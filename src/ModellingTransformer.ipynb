{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vvall\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install keras-self-attention\n",
    "import os, time, gc, sys, io\n",
    "import datetime, pickle, session_info\n",
    "import warnings, random, math\n",
    "#\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mpl_toolkits.axisartist as AA\n",
    "#\n",
    "from scipy import stats\n",
    "from pandas import Series\n",
    "from argparse import ArgumentParser\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "#from keras_self_attention import SeqSelfAttention\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "#\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdat     = '../DataProcessed/15/m-input-output.pkl'\n",
    "file      = open(fdat, 'rb')\n",
    "path      = pickle.load(file)\n",
    "fdat     = pickle.load(file)\n",
    "lahead    = pickle.load(file)\n",
    "lpar      =pickle.load(file)\n",
    "stock_list= pickle.load(file)\n",
    "tot_res   = pickle.load(file)\n",
    "df_dict   = tot_res['INP']\n",
    "win, deep, n_ftrs,tr_tst = lpar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['INP', 'INP_MSERIAL'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07',\n",
       "               '2015-01-08', '2015-01-09', '2015-01-12', '2015-01-13',\n",
       "               '2015-01-14', '2015-01-15',\n",
       "               ...\n",
       "               '2023-12-15', '2023-12-18', '2023-12-19', '2023-12-20',\n",
       "               '2023-12-21', '2023-12-22', '2023-12-26', '2023-12-27',\n",
       "               '2023-12-28', '2023-12-29'],\n",
       "              dtype='datetime64[ns]', name='date', length=2246, freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['TSLA'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (18, 10)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from EncoderTransformer import TransformerEncoder as Encoder\n",
    "from DecoderTransformer import TransformerDecoder as Decoder\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_encoder_layers: int,\n",
    "            num_decoder_layers: int,\n",
    "            dim_model: int,\n",
    "            num_attention_heads: int,\n",
    "            units_hidden_layer: int,\n",
    "            dropout: float,\n",
    "            n_ftrs: int,\n",
    "            activation: nn.Module = nn.ReLU(),\n",
    "            mask: bool = True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder (\n",
    "            num_layers=num_encoder_layers,\n",
    "            d_model=dim_model,\n",
    "            num_heads=num_attention_heads,\n",
    "            units_hidden_layer=units_hidden_layer,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            mask=mask)\n",
    "\n",
    "        self.decoder = Decoder (\n",
    "            num_layers=num_decoder_layers,\n",
    "            d_model=dim_model,\n",
    "            num_heads=num_attention_heads,\n",
    "            units_hidden_layer=units_hidden_layer,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            mask=mask)\n",
    "        \n",
    "        self.output_layer = nn.Linear(dim_model, n_ftrs) # Linear layer for predicting the single feature\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        outputs = []\n",
    "\n",
    "        for src, tgt_seq in tqdm(zip(source, target), total=len(source)):\n",
    "            src = torch.unsqueeze(src, dim=0)\n",
    "            encoder_output = self.encoder(src)\n",
    "\n",
    "            batch_output = []\n",
    "            for tgt_step in tgt_seq:\n",
    "                tgt_step = torch.unsqueeze(tgt_step, dim=0)\n",
    "                decoder_output = self.decoder(src=tgt_step, memory=encoder_output)\n",
    "                batch_output.append(decoder_output)\n",
    "\n",
    "            # Concatenate along the time dimension and apply the linear layer\n",
    "            output = torch.cat(batch_output, dim=0)\n",
    "            output = self.output_layer(output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return torch.cat(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "units_hidden_layer = 512\n",
    "dropout_rate = 0.1\n",
    "\n",
    "epoch = 200\n",
    "bsize = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "def transformer_fun(trainX, trainY, testX, testY, Y, vdd, epoch, bsize, nhead, num_encoder_layers, num_decoder_layers, win, n_ftrs, shft, stock, seed):\n",
    "    nptrX = trainX #.reshape(trainX.shape[0], trainX.shape[1], n_ftrs)\n",
    "    nptrY = trainY\n",
    "    nit = 0\n",
    "    lloss = np.nan\n",
    "    \n",
    "    while math.isnan(lloss) and nit < 5:\n",
    "        tf.random.set_seed(seed)\n",
    "        # create a Transformer model\n",
    "        transformer_model = Transformer(\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_model=d_model,\n",
    "        num_attention_heads=num_heads,\n",
    "        units_hidden_layer=units_hidden_layer,\n",
    "        dropout=dropout_rate,\n",
    "        n_ftrs=n_ftrs\n",
    "        ) # Instantiate the Transformer model\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.TimeDistributed(transformer_model, input_shape=(nptrX.shape[0], nptrX.shape[1])))\n",
    "        model.add(layers.GlobalAveragePooling1D())\n",
    "        model.add(layers.Dense(n_ftrs))  # Output of a single value\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        # Training\n",
    "        hist = model.fit(nptrX, nptrY, epochs=epoch, batch_size=bsize, verbose=0)\n",
    "        lloss = hist.history['loss'][-1]\n",
    "        nit = nit + 1\n",
    "\n",
    "    # Predict\n",
    "    nptstX = testX.to_numpy().reshape(testX.shape[0], testX.shape[1], n_ftrs)\n",
    "    nptstY = testY.to_numpy()\n",
    "    res1 = eval(nptstX, nptstY, model, vdd, Y, shft)\n",
    "    \n",
    "    df_result = {\n",
    "        'MSEP': res1.get(\"msep\"),\n",
    "        'MSEY': res1.get(\"msey\"),\n",
    "        'Stock': stock,\n",
    "        'DY': res1.get(\"Ys\"),\n",
    "        'ALG': 'Transformer',\n",
    "        'seed': seed,\n",
    "        'epochs': epoch,\n",
    "        'num_head': nhead,\n",
    "        'win': win,\n",
    "        'ndims': 1,\n",
    "        'lossh': lloss,\n",
    "        'nit': nit,\n",
    "        'model': model\n",
    "    }\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1779, 15, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_res['INP_MSERIAL'][\"TSLA\"][7]['trX'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. Received: Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerEncoderLayer(\n        (attention): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerDecoderLayer(\n        (attention_1): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (attention_2): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [54], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m mdl_name  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:03}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:02}\u001b[39;00m\u001b[38;5;124m.hd5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tmod,stock,ahead,irp)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 25\u001b[0m     sol   \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_encoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_decoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_ftrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mahead\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstock\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m tranf_end  \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     27\u001b[0m ttrain    \u001b[38;5;241m=\u001b[39m tranf_end \u001b[38;5;241m-\u001b[39m transf_start\n",
      "Cell \u001b[1;32mIn [51], line 44\u001b[0m, in \u001b[0;36mtransformer_fun\u001b[1;34m(trainX, trainY, testX, testY, Y, vdd, epoch, bsize, nhead, num_encoder_layers, num_decoder_layers, win, n_ftrs, shft, stock, seed)\u001b[0m\n\u001b[0;32m     33\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m     34\u001b[0m num_encoder_layers\u001b[38;5;241m=\u001b[39mnum_encoder_layers,\n\u001b[0;32m     35\u001b[0m num_decoder_layers\u001b[38;5;241m=\u001b[39mnum_decoder_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m n_ftrs\u001b[38;5;241m=\u001b[39mn_ftrs\n\u001b[0;32m     41\u001b[0m ) \u001b[38;5;66;03m# Instantiate the Transformer model\u001b[39;00m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m---> 44\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnptrX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnptrX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling1D())\n\u001b[0;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(n_ftrs))  \u001b[38;5;66;03m# Output of a single value\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py:74\u001b[0m, in \u001b[0;36mTimeDistributed.__init__\u001b[1;34m(self, layer, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Layer):\n\u001b[1;32m---> 74\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease initialize `TimeDistributed` layer with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.layers.Layer` instance. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_masking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. Received: Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerEncoderLayer(\n        (attention): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerDecoderLayer(\n        (attention_1): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (attention_2): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n)"
     ]
    }
   ],
   "source": [
    "res  = {}\n",
    "tmod =  \"transformer\"\n",
    "res['MODEL'] = tmod\n",
    "fmdls= '/{}/Models/'.format(win)+tmod+'/'\n",
    "if not os.path.exists(fmdls):\n",
    "    os.makedirs(fmdls)\n",
    "#\n",
    "for stock in stock_list:\n",
    "    res[stock] = {}\n",
    "    for ahead in lahead:\n",
    "        # print('Training ' + stock)\n",
    "        trainX = tot_res['INP_MSERIAL'][stock][ahead]['trX']\n",
    "        trainY = tot_res['INP_MSERIAL'][stock][ahead]['trY']\n",
    "        testX  = tot_res['INP_MSERIAL'][stock][ahead]['tsX']    \n",
    "        testY  = tot_res['INP_MSERIAL'][stock][ahead]['tsY']\n",
    "        Y      = tot_res['INP_MSERIAL'][stock][ahead]['y']\n",
    "        vdd    = tot_res['INP_MSERIAL'][stock][ahead]['vdd']\n",
    "        tmpr = []\n",
    "        \n",
    "        for irp in range(10):\n",
    "            seed = random.randint(0,1000)\n",
    "            transf_start= time.time()\n",
    "            mdl_name  = '{}-{}-{:03}-{:02}.hd5'.format(tmod,stock,ahead,irp)\n",
    "            if tmod == \"transformer\":\n",
    "                sol   = transformer_fun(trainX,trainY,testX,testY,Y,vdd,epoch,bsize,num_heads,num_encoder_layers,num_decoder_layers,win,n_ftrs,ahead,stock,seed)\n",
    "            tranf_end  = time.time()\n",
    "            ttrain    = tranf_end - transf_start\n",
    "            sol['ttrain'] = ttrain\n",
    "            sol['epoch']  = epoch\n",
    "            sol['bsize']  = bsize\n",
    "            sol['num_heads']    = num_heads\n",
    "            sol['model'].save(fmdls+mdl_name)\n",
    "            sol['model']  = fmdls+mdl_name\n",
    "            # print('   Effort spent: ' + str(ttrain) +' s.')\n",
    "            # sys.stdout.flush()\n",
    "            tmpr.append(sol)\n",
    "        res[stock][ahead] = pd.DataFrame(tmpr)\n",
    "#\n",
    "tot_res['OUT_MODEL'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
