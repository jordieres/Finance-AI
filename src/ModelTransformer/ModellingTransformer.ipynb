{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vvall\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install keras-self-attention\n",
    "import os, time, gc, sys, io\n",
    "import datetime, pickle, session_info\n",
    "import warnings, random, math\n",
    "#\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mpl_toolkits.axisartist as AA\n",
    "#\n",
    "from scipy import stats\n",
    "from pandas import Series\n",
    "from argparse import ArgumentParser\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "#from keras_self_attention import SeqSelfAttention\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "#\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdat     = '../DataProcessed/15/m-input-output.pkl'\n",
    "file      = open(fdat, 'rb')\n",
    "path      = pickle.load(file)\n",
    "fdat     = pickle.load(file)\n",
    "lahead    = pickle.load(file)\n",
    "lpar      =pickle.load(file)\n",
    "stock_list= pickle.load(file)\n",
    "tot_res   = pickle.load(file)\n",
    "df_dict   = tot_res['INP']\n",
    "win, deep, n_ftrs,tr_tst = lpar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['INP', 'INP_MSERIAL'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07',\n",
       "               '2015-01-08', '2015-01-09', '2015-01-12', '2015-01-13',\n",
       "               '2015-01-14', '2015-01-15',\n",
       "               ...\n",
       "               '2023-12-15', '2023-12-18', '2023-12-19', '2023-12-20',\n",
       "               '2023-12-21', '2023-12-22', '2023-12-26', '2023-12-27',\n",
       "               '2023-12-28', '2023-12-29'],\n",
       "              dtype='datetime64[ns]', name='date', length=2246, freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['TSLA'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (18, 10)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "d_model = 10 # multivariate number of features considered\n",
    "num_heads = 8\n",
    "units_hidden_layer = 512\n",
    "dropout_rate = 0.1\n",
    "\n",
    "epochs = 200\n",
    "bsize = 75\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor, float32, uint8\n",
    "\n",
    "def training_transformer(model, optimizer, criterion, train_X, train_Y, test_X, test_Y, epochs, train_loss=None,\n",
    "                         test_loss=None):\n",
    "    if train_loss is not None:\n",
    "        epoch_loss_train = train_loss\n",
    "        best_train_loss = min([np.mean(i) for i in train_loss])\n",
    "        best_epoch = np.where(min([np.mean(i) for i in test_loss]))\n",
    "    else:\n",
    "        epoch_loss_train = []\n",
    "        best_train_loss = 9999999\n",
    "        best_epoch = 0\n",
    "\n",
    "    if test_loss is not None:\n",
    "        epoch_loss_test = test_loss\n",
    "        best_test_loss = min([np.mean(i) for i in test_loss])\n",
    "        best_epoch = np.where(min([np.mean(i) for i in train_loss]))\n",
    "    else:\n",
    "        epoch_loss_test = []\n",
    "        best_test_loss = 9999999\n",
    "        best_epoch = 0\n",
    "\n",
    "    transformer_model = model\n",
    "    starting_epoch = len(epoch_loss_test)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        print(f'Epoch: {epoch + starting_epoch} of {epochs}')\n",
    "        print('Training...')\n",
    "        model.train()\n",
    "\n",
    "        for i in tqdm(train_X):\n",
    "            input = train_X[i]\n",
    "            target = train_Y[i]\n",
    "\n",
    "            transformer_out = model.forward(input, target)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(transformer_out, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "            # Optimization\n",
    "            optimizer.step()\n",
    "\n",
    "        print('\\nTesting with training data')\n",
    "        losses_train = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(train_X):\n",
    "                input = train_X[i]\n",
    "                target = train_Y[i]\n",
    "\n",
    "                transformer_out = model.forward(input, target)\n",
    "\n",
    "                # Compute loss\n",
    "                losses_train.append(float(criterion(transformer_out, target).item()))\n",
    "\n",
    "        print('\\nCurrent Mean loss Train Data: ', np.mean(losses_train))\n",
    "        epoch_loss_train.append(losses_train)\n",
    "\n",
    "        print('\\nTest with test data')\n",
    "        losses_test = []\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(test_X):\n",
    "                input = test_X[i]\n",
    "                target = test_Y[i]\n",
    "\n",
    "                transformer_out = model.forward(input, target)\n",
    "\n",
    "                # Compute loss\n",
    "                losses_test.append(float(criterion(transformer_out, target).item()))\n",
    "\n",
    "        print('\\nCurrent Mean loss Test Data: ', np.mean(losses_test))\n",
    "        epoch_loss_test.append(losses_test)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        if np.mean(losses_test) < best_test_loss:\n",
    "            best_test_loss = np.mean(losses_test)\n",
    "            best_train_loss = np.mean(losses_train)\n",
    "            best_model = transformer_model\n",
    "            best_epoch = epoch\n",
    "\n",
    "    return (best_model, best_train_loss, best_test_loss, best_epoch), epoch_loss_train, epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from Transformer import Transformer\n",
    "\n",
    "\n",
    "def transformer_fun(trainX, trainY, testX, testY, Y, vdd, epochs, bsize, nhead, num_encoder_layers, num_decoder_layers, win, n_ftrs, shft, stock, seed):\n",
    "    nptrX = trainX #.reshape(trainX.shape[0], trainX.shape[1], n_ftrs)\n",
    "    nptrY = trainY\n",
    "    nit = 0\n",
    "    lloss = np.nan\n",
    "    \n",
    "    while math.isnan(lloss) and nit < 5:\n",
    "        tf.random.set_seed(seed)\n",
    "        # create a Transformer model\n",
    "        transformer_model = Transformer(\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_model=d_model,\n",
    "        num_attention_heads=num_heads,\n",
    "        units_hidden_layer=units_hidden_layer,\n",
    "        dropout=dropout_rate,\n",
    "        n_ftrs=n_ftrs\n",
    "        ) # Instantiate the Transformer model\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(transformer_model.parameters(), lr=learning_rate)\n",
    "\n",
    "        best_results, train_losses, test_losses = training_transformer(\n",
    "                model=transformer_model,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                train_X=trainX,\n",
    "                train_Y=trainY,\n",
    "                test_X=testX,\n",
    "                test_Y=testY,\n",
    "                n_epochs=epochs)\n",
    "\n",
    "\n",
    "\n",
    "    # Predict\n",
    "    nptstX = testX.to_numpy().reshape(testX.shape[0], testX.shape[1], n_ftrs)\n",
    "    nptstY = testY.to_numpy()\n",
    "    res1 = eval(nptstX, nptstY, model, vdd, Y, shft)\n",
    "    \n",
    "    df_result = {\n",
    "        'MSEP': res1.get(\"msep\"),\n",
    "        'MSEY': res1.get(\"msey\"),\n",
    "        'Stock': stock,\n",
    "        'DY': res1.get(\"Ys\"),\n",
    "        'ALG': 'Transformer',\n",
    "        'seed': seed,\n",
    "        'epochs': epoch,\n",
    "        'num_head': nhead,\n",
    "        'win': win,\n",
    "        'ndims': 1,\n",
    "        'lossh': lloss,\n",
    "        'nit': nit,\n",
    "        'model': model\n",
    "    }\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1779, 15, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_res['INP_MSERIAL'][\"TSLA\"][7]['trX'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vvall\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. Received: Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerEncoderLayer(\n        (attention): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerDecoderLayer(\n        (attention_1): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (attention_2): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m mdl_name  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:03}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:02}\u001b[39;00m\u001b[38;5;124m.hd5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tmod,stock,ahead,irp)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 25\u001b[0m     sol   \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_encoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_decoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_ftrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mahead\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstock\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m tranf_end  \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     27\u001b[0m ttrain    \u001b[38;5;241m=\u001b[39m tranf_end \u001b[38;5;241m-\u001b[39m transf_start\n",
      "Cell \u001b[1;32mIn [8], line 27\u001b[0m, in \u001b[0;36mtransformer_fun\u001b[1;34m(trainX, trainY, testX, testY, Y, vdd, epoch, bsize, nhead, num_encoder_layers, num_decoder_layers, win, n_ftrs, shft, stock, seed)\u001b[0m\n\u001b[0;32m     16\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m     17\u001b[0m num_encoder_layers\u001b[38;5;241m=\u001b[39mnum_encoder_layers,\n\u001b[0;32m     18\u001b[0m num_decoder_layers\u001b[38;5;241m=\u001b[39mnum_decoder_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m n_ftrs\u001b[38;5;241m=\u001b[39mn_ftrs\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;66;03m# Instantiate the Transformer model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m---> 27\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnptrX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnptrX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling1D())\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(n_ftrs))  \u001b[38;5;66;03m# Output of a single value\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py:74\u001b[0m, in \u001b[0;36mTimeDistributed.__init__\u001b[1;34m(self, layer, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Layer):\n\u001b[1;32m---> 74\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease initialize `TimeDistributed` layer with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.layers.Layer` instance. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_masking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. Received: Transformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerEncoderLayer(\n        (attention): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerDecoderLayer(\n        (attention_1): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (attention_2): Residual(\n          (sublayer): MultiHeadAttentionLayer(\n            (attention_heads): ModuleList(\n              (0-7): 8 x AttentionHead(\n                (query): Linear(in_features=256, out_features=32, bias=True)\n                (key): Linear(in_features=256, out_features=32, bias=True)\n                (value): Linear(in_features=256, out_features=32, bias=True)\n              )\n            )\n            (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): Residual(\n          (sublayer): Sequential(\n            (0): Linear(in_features=256, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=256, bias=True)\n          )\n          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (output_layer): Linear(in_features=256, out_features=1, bias=True)\n)"
     ]
    }
   ],
   "source": [
    "res  = {}\n",
    "tmod =  \"transformer\"\n",
    "res['MODEL'] = tmod\n",
    "fmdls= '/{}/Models/'.format(win)+tmod+'/'\n",
    "if not os.path.exists(fmdls):\n",
    "    os.makedirs(fmdls)\n",
    "#\n",
    "for stock in stock_list:\n",
    "    res[stock] = {}\n",
    "    for ahead in lahead:\n",
    "        # print('Training ' + stock)\n",
    "        trainX = tot_res['INP_MSERIAL'][stock][ahead]['trX']\n",
    "        trainY = tot_res['INP_MSERIAL'][stock][ahead]['trY']\n",
    "        testX  = tot_res['INP_MSERIAL'][stock][ahead]['tsX']    \n",
    "        testY  = tot_res['INP_MSERIAL'][stock][ahead]['tsY']\n",
    "        Y      = tot_res['INP_MSERIAL'][stock][ahead]['y']\n",
    "        vdd    = tot_res['INP_MSERIAL'][stock][ahead]['vdd']\n",
    "        tmpr = []\n",
    "        \n",
    "        for irp in range(10):\n",
    "            seed = random.randint(0,1000)\n",
    "            transf_start= time.time()\n",
    "            mdl_name  = '{}-{}-{:03}-{:02}.hd5'.format(tmod,stock,ahead,irp)\n",
    "            if tmod == \"transformer\":\n",
    "                sol   = transformer_fun(trainX,trainY,testX,testY,Y,vdd,epoch,bsize,num_heads,num_encoder_layers,num_decoder_layers,win,n_ftrs,ahead,stock,seed)\n",
    "            tranf_end  = time.time()\n",
    "            ttrain    = tranf_end - transf_start\n",
    "            sol['ttrain'] = ttrain\n",
    "            sol['epoch']  = epoch\n",
    "            sol['bsize']  = bsize\n",
    "            sol['num_heads']    = num_heads\n",
    "            sol['model'].save(fmdls+mdl_name)\n",
    "            sol['model']  = fmdls+mdl_name\n",
    "            # print('   Effort spent: ' + str(ttrain) +' s.')\n",
    "            # sys.stdout.flush()\n",
    "            tmpr.append(sol)\n",
    "        res[stock][ahead] = pd.DataFrame(tmpr)\n",
    "#\n",
    "tot_res['OUT_MODEL'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
